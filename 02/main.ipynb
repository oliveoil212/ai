{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas 1.5.2 requires numpy>=1.20.3; python_version < \"3.10\", but you have numpy 1.19.5 which is incompatible.\n",
    "ImportError: this version of pandas is incompatible with numpy < 1.20.3\n",
    "your numpy version is 1.19.5.\n",
    "Please upgrade numpy to >= 1.20.3 to use this pandas version\n",
    "\n",
    "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "simcse 0.4 requires numpy<1.20,>=1.19.5, but you have numpy 1.20.3 which is incompatible.\n",
    "\n",
    "\n",
    "conda search --info pandas | grep -E '(^version|numpy)'\n",
    "version     : 1.3.4\n",
    "  - numpy >=1.19.5,<2.0a0\n",
    "\n",
    "en un mot, downgrade pandas to 1.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade numpy==1.19.5\n",
    "# !pip install --upgrade pandas==1.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "import json\n",
    "\n",
    "from simcse import SimCSE\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = SimCSE(\"princeton-nlp/sup-simcse-roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### load and prepocess raw data\n",
    "# def make_sentence(row):\n",
    "#     return ' '.join([str(row['head_name']),str(row['relation']),str(row['tail_name'])])\n",
    "# def trim_relations(rel):\n",
    "#     return ', '.join(rel.split('/')[1:])\n",
    "    \n",
    "# df = pd.read_json('./train_df.json')\n",
    "# df['relation'] = df['relation'].apply(trim_relations)\n",
    "# df['sentence'] = df.apply(make_sentence, axis=1)\n",
    "# # df['tokens'] = df.apply(lambda hrt: tokenizer.tokenize(hrt['sentence']),axis = 1) # padas dataframe doesn't support list datatype.\n",
    "# tokenize_sentences =  {str(index): tokenizer.tokenize(sentence) for index, sentence in df['sentence'].items()}\n",
    "\n",
    "# #### save data\n",
    "# df.to_csv('./train_df_token.csv',index=False) \n",
    "# with open('tokenize_sentences.json', 'w') as fp:\n",
    "#     json.dump(tokenize_sentences, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data\n",
    "df = pd.read_csv('./train_df_token.csv')\n",
    "with open('tokenize_sentences.json', 'r') as fp:\n",
    "    tokenize_sentences = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_len = pd.Series({index: len(tokens) for index, tokens in tokenize_sentences.items()})\n",
    "dict_merged_tokens = {index: str(hrt) for index, hrt in tokenize_sentences.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已知一個hrt，假設tail被mask。\n",
    "找他的one hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "hrt = df.iloc[i] # a given hrt. Assume tail is masked\n",
    "\n",
    "#### find 1-hops\n",
    "one_hops = df[(df.tail_code == hrt['tail_code']) & (df.index != i)] # itself is not its 1-hop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token數量不能大於512，\n",
    "所以如果大於512，讓one hops 和hrt比較similarity，從高similarity的優先取，取到超過(一超過就不取了，沒有再去看低similarity的onehop有沒有符合)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_size =  tokens_len[i] + tokens_len[one_hops.index].sum() + len(one_hops.index)\n",
    "# token_size =  hrt['tokens_len'] + one_hops['tokens_len'].sum() + len(one_hops.index)\n",
    "if token_size > 512:    # check if token size is greater than 512, choose those one hops with high similarity\n",
    "    one_hops['similarity'] = one_hops.apply(lambda row: model.similarity(hrt['sentence'], row['sentence']),axis=1)\n",
    "    one_hops = one_hops.sort_values(by='similarity',ascending=False)\n",
    "    accum =  tokens_len[one_hops.index].cumsum()\n",
    "    sep = pd.Series(list(range(1, accum.size+1)), index=accum.index) # use n [SEP] tokens to seperate n one-hop\n",
    "    accum = accum.add(sep).add(tokens_len[i])\n",
    "    \n",
    "one_hops=one_hops[(accum <= 512).values]        # filter out\n",
    "token_size_after =  tokens_len[i] + tokens_len[one_hops.index].sum() + len(one_hops.index) # check again\n",
    "# if token_size_after >= 512:\n",
    "#     print('error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hrt間用[SEP]區隔最後輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hrts = hrt['tokens'] + [token for token_list in one_hops['tokens'].tolist() for token in token_list]\n",
    "## Given a list of lists l, flat_list = [item for sublist in l for item in sublist]\n",
    "# hrts = tokenize_sentences[1] + [tokens  for index in one_hops.index for tokens in tokenize_sentences[index]]\n",
    "# dict_merged_tokens = {index: '[D]'.join(hrt) for index, hrt in tokenize_sentences.items()}\n",
    "\n",
    "hrts_token =  [dict_merged_tokens[str(i)]] + [dict_merged_tokens[str(index)]  for index in one_hops.index]\n",
    "output = '[SEP] \\n'.join(hrts_token) # seperate hrts with [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('sim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8ea0479ffe93b0ef389cc9d445072f7c57b9b0bc9d4c0e236c6c5b788df5f2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
